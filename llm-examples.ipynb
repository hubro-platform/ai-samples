{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hubro Platform - AI integration examples\n",
    "\n",
    "This notebook demonstrates multiple approaches of integrating LLM or classic ML algorithms into your Hubro study.\n",
    "Preferably, run this notebook in a prepared environment as documented in http://docs.hubroplatform.no/ai-inference.html#preparing-your-environment"
   ],
   "id": "691ddf0fcf96357a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before continuing, make sure that your MLFlow server is running locally on port 5000\n",
    " \n",
    "```mlflow server```\n",
    "\n",
    "Run the following cell to install dependencies and set up API keys to the LLM engine of your choice."
   ],
   "id": "ea4f3c5de46a28c3"
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install openai langchain langchain_community openai langchain_openai replicate scikit-learn pandas\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=\"YOUR OPENAI API KEY\"\n",
    "os.environ['REPLICATE_API_TOKEN']=\"YOUR REPLICATE API TOKEN\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-07T05:29:24.191630Z",
     "start_time": "2024-09-07T05:29:24.187122Z"
    }
   },
   "id": "b08ba93976e9f15a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The example below demonstrates use of a prompt template-based LLMChain using OpenAI's ChatGPT. ",
   "id": "92918458285eb1b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T10:01:05.568481Z",
     "start_time": "2024-09-06T10:01:02.103705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#source: https://mlflow.org/docs/latest/python_api/mlflow.langchain.html\n",
    "from langchain_community.llms.openai import OpenAI\n",
    "import mlflow\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import Mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"LLMChain using OpenAI\")\n",
    "\n",
    "assert \"OPENAI_API_TOKEN\" in os.environ, \"Please set the OPENAI_API_TOKEN environment variable.\"\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"adjective\"],\n",
    "        template=\"Tell me a {adjective} joke\",\n",
    "    ),\n",
    ")\n",
    "with mlflow.start_run():\n",
    "    mlflow.langchain.log_model(llm_chain, \"openai_joke_generator\")"
   ],
   "id": "5affffc62d9d1203",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/06 12:01:05 INFO mlflow.tracking._tracking_service.client: üèÉ View run masked-turtle-391 at: http://127.0.0.1:5000/#/experiments/965190641623225329/runs/0542ecbf8b734a938c5d6197d992e7a6.\n",
      "2024/09/06 12:01:05 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/965190641623225329.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model should be tracked by now. You can view the run by visiting MLFlow's UI http://127.0.0.1:5000 in the browser. The model is also now ready to be deployed to Hubro Platform. As explained in https://docs.hubroplatform.no/ai-inference.html#deploying-models-to-hubro and documented later in this notebook, the basic two options for model deployment are using KServe or KNative.",
   "id": "f9612ec6bc3e777e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another example below demonstrates creation of a simple answering using LCEL definition using another LLM provider (Replicate). This example also makes use of model autologging feature of MLFlow. ",
   "id": "27e1d23c6c10d3ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T06:24:13.922198Z",
     "start_time": "2024-09-08T06:23:54.878121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#https://mlflow.org/docs/latest/llms/langchain/autologging.html#example-code-of-langchain-autologging\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain_community.llms import Replicate\n",
    "import mlflow  \n",
    "import replicate\n",
    "\n",
    "assert \"REPLICATE_API_TOKEN\" in os.environ, \"Please set the REPLICATE_API_TOKEN environment variable.\"\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"LCEL using Replicate\")\n",
    "\n",
    "# Enable mlflow langchain autologging\n",
    "mlflow.langchain.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=False,\n",
    "    log_models=True,\n",
    "    log_inputs_outputs=True,\n",
    "    registered_model_name=\"replicate_answering_model\",\n",
    ")\n",
    "\n",
    "prompt_with_history_str = \"\"\"\n",
    "Please answer this question: {question}\n",
    "\"\"\"\n",
    "prompt_with_history = PromptTemplate(\n",
    "    input_variables=[\"query\"], template=prompt_with_history_str\n",
    ")\n",
    "\n",
    "def extract_question(input):\n",
    "    return input\n",
    "\n",
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-70b-instruct\",\n",
    "    model_kwargs={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1},\n",
    ")\n",
    "\n",
    "chain_with_history = (\n",
    "        {\n",
    "            \"question\": itemgetter(\"query\") | RunnableLambda(extract_question),\n",
    "        }\n",
    "        | prompt_with_history\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "inputs = {\"query\": \"What is the highest mountain in the world?\"}\n",
    "\n",
    "print(chain_with_history.invoke(inputs))\n",
    "\n",
    "# Verification can be done using\n",
    "\n",
    "model_name = \"replicate_answering_model\"\n",
    "model_version = 1\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "print(loaded_model.predict(inputs))"
   ],
   "id": "aee747411dd643fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/08 08:23:54 WARNING mlflow.utils.autologging_utils.config: The log_inputs_outputs option is deprecated and will be removed in a future release. Please use the log_traces option in `mlflow.<flavor>.autolog` to log traces (including inputs and outputs) of the model.\n",
      "2024/09/08 08:24:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/_langchain_autolog.py:363: UserWarning: MLflow autologging does not support logging models containing BaseRetriever because logging the model requires `loader_fn` and `persist_dir`. Please log the model manually using `mlflow.langchain.log_model(model, artifact_path, loader_fn=..., persist_dir=...)`\"\n",
      "2024/09/08 08:24:06 INFO mlflow.langchain._langchain_autolog: Signature is automatically generated for logged model if input_example is provided. To disable log_model_signatures, please also disable log_input_examples.\n",
      "2024/09/08 08:24:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/models/utils.py:523: FutureWarning: Since MLflow 2.16.0, we no longer convert dictionary input example to pandas Dataframe, and directly save it as a json object. If the model expects a pandas DataFrame input instead, please pass the pandas DataFrame as input example directly.\"\n",
      "2024/09/08 08:24:06 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException('1 tasks failed. Errors: {0: \\'error: ReplicateError(type=None, title=\\\\\\'Free time limit reached\\\\\\', status=402, detail=\\\\\\'You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\\\\\', instance=None) Traceback (most recent call last):\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 194, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 145, in single_call_api\\\\n    response = self._predict_single_input(self.request_json, callback_handlers)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 115, in _predict_single_input\\\\n    return self.lc_model.invoke(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 460, in safe_patch_function\\\\n    return original(*args, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2878, in invoke\\\\n    input = context.run(step.invoke, input, config)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 460, in safe_patch_function\\\\n    return original(*args, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 385, in invoke\\\\n    self.generate_prompt(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 750, in generate_prompt\\\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 944, in generate\\\\n    output = self._generate_helper(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 787, in _generate_helper\\\\n    raise e\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 774, in _generate_helper\\\\n    self._generate(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1508, in _generate\\\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 139, in _call\\\\n    prediction = self._create_prediction(prompt, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 226, in _create_prediction\\\\n    return replicate_python.models.predictions.create(self.model, input=input_)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/model.py\", line 405, in create\\\\n    resp = self._client._request(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 89, in _request\\\\n    _raise_for_status(resp)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 383, in _raise_for_status\\\\n    raise ReplicateError.from_response(resp)\\\\nreplicate.exceptions.ReplicateError: ReplicateError Details:\\\\ntitle: Free time limit reached\\\\nstatus: 402\\\\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\\\n\\\\n request payload: {\\\\\\'query\\\\\\': \\\\\\'How can I conmfort myself during my period?\\\\\\'}\\'}'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`. To disable automatic signature inference, set `signature` to `False` in your `log_model` or `save_model` call.\n",
      "2024/09/08 08:24:09 WARNING mlflow.utils.requirements_utils: Failed to run predict on input_example, dependencies introduced in predict are not captured.\n",
      "MlflowException('1 tasks failed. Errors: {0: \\'error: ReplicateError(type=None, title=\\\\\\'Free time limit reached\\\\\\', status=402, detail=\\\\\\'You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\\\\\', instance=None) Traceback (most recent call last):\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 194, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 145, in single_call_api\\\\n    response = self._predict_single_input(self.request_json, callback_handlers)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 115, in _predict_single_input\\\\n    return self.lc_model.invoke(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2878, in invoke\\\\n    input = context.run(step.invoke, input, config)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 385, in invoke\\\\n    self.generate_prompt(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 750, in generate_prompt\\\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 944, in generate\\\\n    output = self._generate_helper(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 787, in _generate_helper\\\\n    raise e\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 774, in _generate_helper\\\\n    self._generate(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1508, in _generate\\\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 139, in _call\\\\n    prediction = self._create_prediction(prompt, **kwargs)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 226, in _create_prediction\\\\n    return replicate_python.models.predictions.create(self.model, input=input_)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/model.py\", line 405, in create\\\\n    resp = self._client._request(\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 89, in _request\\\\n    _raise_for_status(resp)\\\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 383, in _raise_for_status\\\\n    raise ReplicateError.from_response(resp)\\\\nreplicate.exceptions.ReplicateError: ReplicateError Details:\\\\ntitle: Free time limit reached\\\\nstatus: 402\\\\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\\\n\\\\n request payload: {\\\\\\'query\\\\\\': \\\\\\'How can I conmfort myself during my period?\\\\\\'}\\'}')Traceback (most recent call last):\n",
      "\n",
      "\n",
      "  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/utils/_capture_modules.py\", line 165, in load_model_and_predict\n",
      "    model.predict(input_example, params=params)\n",
      "\n",
      "\n",
      "  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/__init__.py\", line 665, in predict\n",
      "    return self._predict_with_callbacks(data, params, callback_handlers=callbacks)\n",
      "\n",
      "\n",
      "  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/__init__.py\", line 713, in _predict_with_callbacks\n",
      "    results = process_api_requests(\n",
      "\n",
      "\n",
      "  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 278, in process_api_requests\n",
      "    raise mlflow.MlflowException(\n",
      "\n",
      "\n",
      "mlflow.exceptions.MlflowException: 1 tasks failed. Errors: {0: 'error: ReplicateError(type=None, title=\\'Free time limit reached\\', status=402, detail=\\'You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\', instance=None) Traceback (most recent call last):\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 194, in call_api\\n    response = self.single_call_api(callback_handlers)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 145, in single_call_api\\n    response = self._predict_single_input(self.request_json, callback_handlers)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 115, in _predict_single_input\\n    return self.lc_model.invoke(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2878, in invoke\\n    input = context.run(step.invoke, input, config)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 385, in invoke\\n    self.generate_prompt(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 750, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 944, in generate\\n    output = self._generate_helper(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 787, in _generate_helper\\n    raise e\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 774, in _generate_helper\\n    self._generate(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1508, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 139, in _call\\n    prediction = self._create_prediction(prompt, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 226, in _create_prediction\\n    return replicate_python.models.predictions.create(self.model, input=input_)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/model.py\", line 405, in create\\n    resp = self._client._request(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 89, in _request\\n    _raise_for_status(resp)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 383, in _raise_for_status\\n    raise ReplicateError.from_response(resp)\\nreplicate.exceptions.ReplicateError: ReplicateError Details:\\ntitle: Free time limit reached\\nstatus: 402\\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\n\\n request payload: {\\'query\\': \\'How can I conmfort myself during my period?\\'}'}\n",
      "Registered model 'replicate_test_model' already exists. Creating a new version of this model...\n",
      "2024/09/08 08:24:11 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: replicate_test_model, version 5\n",
      "Created version '5' of model 'replicate_test_model'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "904a23c272f24507bc10589f883d033c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/08 08:24:13 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": {\n",
      "    \"query\": \"How can I conmfort myself during my period?\"\n",
      "  }\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: 1 tasks failed. Errors: {0: 'error: ReplicateError(type=None, title=\\'Free time limit reached\\', status=402, detail=\\'You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\', instance=None) Traceback (most recent call last):\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 194, in call_api\\n    response = self.single_call_api(callback_handlers)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 145, in single_call_api\\n    response = self._predict_single_input(self.request_json, callback_handlers)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 115, in _predict_single_input\\n    return self.lc_model.invoke(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 460, in safe_patch_function\\n    return original(*args, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2878, in invoke\\n    input = context.run(step.invoke, input, config)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 460, in safe_patch_function\\n    return original(*args, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 385, in invoke\\n    self.generate_prompt(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 750, in generate_prompt\\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 944, in generate\\n    output = self._generate_helper(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 787, in _generate_helper\\n    raise e\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 774, in _generate_helper\\n    self._generate(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1508, in _generate\\n    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 139, in _call\\n    prediction = self._create_prediction(prompt, **kwargs)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/llms/replicate.py\", line 226, in _create_prediction\\n    return replicate_python.models.predictions.create(self.model, input=input_)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/model.py\", line 405, in create\\n    resp = self._client._request(\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 89, in _request\\n    _raise_for_status(resp)\\n  File \"/opt/homebrew/anaconda3/envs/mlflow-env/lib/python3.10/site-packages/replicate/client.py\", line 383, in _raise_for_status\\n    raise ReplicateError.from_response(resp)\\nreplicate.exceptions.ReplicateError: ReplicateError Details:\\ntitle: Free time limit reached\\nstatus: 402\\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.\\n\\n request payload: {\\'query\\': \\'How can I conmfort myself during my period?\\'}'}\n",
      "2024/09/08 08:24:13 INFO mlflow.tracking._tracking_service.client: üèÉ View run langchain-selective-stoat-136 at: http://127.0.0.1:5000/#/experiments/608996866007489115/runs/cac766a07ae1448e9b0f5ab41542c10a.\n",
      "2024/09/08 08:24:13 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/608996866007489115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm happy to help!\n",
      "\n",
      "Dealing with periods can be uncomfortable, both physically and emotionally. Here are some tips to help you comfort yourself during your period:\n",
      "\n",
      "1. **Warmth therapy**: Apply a warm heating pad or a hot water bottle to your lower abdomen to ease cramps. You can also take a warm bath or shower to relax your muscles.\n",
      "2. **Cozy up with comfort foods**: Reach for your favorite comfort foods like chocolate, soup, or mac and cheese. These can help lift your mood and provide a sense of comfort.\n",
      "3. **Stay hydrated**: Drink plenty of water to help flush out toxins and reduce bloating. Herbal teas like chamomile or peppermint can also be soothing.\n",
      "4. **Get comfortable clothes**: Wear loose, comfortable clothing that doesn't irritate your skin. You might find that soft, stretchy fabrics like cotton or modal feel nice against your skin.\n",
      "5. **Pamper yourself**: Treat yourself to a relaxing activity, like reading a book, listening to calming music, or practicing gentle stretches. You can also try face masks, manicures, or other self-care rituals.\n",
      "6. **Take a break**: Allow yourself to take a break from activities that might exacerbate cramps or discomfort. Take a nap, watch a movie, or simply take some time to rest.\n",
      "7. **Practice relaxation techniques**: Deep breathing exercises, meditation, or yoga can help reduce stress and alleviate period symptoms. You can find guided recordings online or through apps like Headspace or Calm.\n",
      "8. **Get moving (gently)**: Engage in gentle exercises like yoga or a short walk to help reduce cramps and improve mood. However, listen to your body and take it easy if you're not feeling up to it.\n",
      "9. **Reach out for support**: Talk to a friend, family member, or partner about how you're feeling. Sometimes, just sharing your emotions with someone who cares can help you feel better.\n",
      "10. **Try over-the-counter pain relievers**: If your cramps are severe, consider taking over-the-counter pain relievers like ibuprofen or acetaminophen. Always follow the recommended dosage and consult with your healthcare provider if necessary.\n",
      "11. **Keep a period journal**: Tracking your symptoms, mood, and energy levels can help you identify patterns and prepare for future periods.\n",
      "12. **Practice self-compassion**: Remind yourself that it's okay to not feel okay during your period. Be kind to yourself, and try not to put\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another example presents an integration of a classic ML classifier.",
   "id": "aa43dc611bce42b4"
  },
  {
   "cell_type": "code",
   "source": [
    "#source: https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html\n",
    "\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Random Forest\")\n",
    "\n",
    "db = load_diabetes()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "# Create and train models.\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "with mlflow.start_run():\n",
    "    rf.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(rf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T19:27:11.293813Z",
     "start_time": "2024-06-18T19:27:03.957510Z"
    }
   },
   "id": "ae7ac112e69d120",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/18 21:27:10 INFO mlflow.tracking.fluent: Experiment with name 'Random Forest' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "log_model() missing 1 required positional argument: 'artifact_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run():\n\u001B[1;32m     18\u001B[0m     rf\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m---> 19\u001B[0m     \u001B[43mmlflow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msklearn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: log_model() missing 1 required positional argument: 'artifact_path'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And yet another example of using Python function as a predictor.",
   "id": "fd7b962df7eaca04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T19:27:21.033525Z",
     "start_time": "2024-06-18T19:27:20.918423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#source: https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Function-based model\")\n",
    "\n",
    "# Define a simple function to log\n",
    "def predict(model_input):\n",
    "    return model_input.apply(lambda x: x * 2)\n",
    "\n",
    "# Save the function as a model\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\"function_model\", python_model=predict, pip_requirements=[\"pandas\"])\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "# mlflow.pyfunc.save(model=\"model\", python_model=predict)\n",
    "\n",
    "model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\")\n",
    "data = pd.Series([1, 2, 3])\n",
    "\n",
    "prediction = model.predict(data)\n",
    "print(prediction)"
   ],
   "id": "284f114fbc889937",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/18 21:27:20 INFO mlflow.tracking.fluent: Experiment with name 'Function-based model 2' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'mlflow.pyfunc' has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 17\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_input\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Save the function as a model\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# with mlflow.start_run():\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#     mlflow.pyfunc.log_model(\"model\", python_model=predict, pip_requirements=[\"pandas\"])\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m#     run_id = mlflow.active_run().info.run_id\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[43mmlflow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpyfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, python_model\u001B[38;5;241m=\u001B[39mpredict)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\")\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# x_new = pd.Series([1, 2, 3])\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# prediction = model.predict(x_new)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# print(prediction)\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/utils/lazy_load.py:42\u001B[0m, in \u001B[0;36mLazyLoader.__getattr__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, item):\n\u001B[1;32m     41\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load()\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'mlflow.pyfunc' has no attribute 'save'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Any of created models can be now served locally for testing using the MLServer. This is a good option for testing the behaviour and performance before putting the model into production. ",
   "id": "beaf59353f830821"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:33:07.664915Z",
     "start_time": "2024-07-17T18:33:03.107974Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/bin/mlflow\", line 8, in <module>\r\n",
      "    sys.exit(cli())\r\n",
      "             ^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\r\n",
      "    return self.main(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/click/core.py\", line 1078, in main\r\n",
      "    rv = self.invoke(ctx)\r\n",
      "         ^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/click/core.py\", line 1688, in invoke\r\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/click/core.py\", line 1688, in invoke\r\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\r\n",
      "    return ctx.invoke(self.callback, **ctx.params)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\r\n",
      "    return __callback(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/models/cli.py\", line 102, in serve\r\n",
      "    return get_flavor_backend(\r\n",
      "           ^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/models/flavor_backend_registry.py\", line 44, in get_flavor_backend\r\n",
      "    local_path = _download_artifact_from_uri(\r\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/tracking/artifact_utils.py\", line 100, in _download_artifact_from_uri\r\n",
      "    return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 117, in get_artifact_repository\r\n",
      "    return _artifact_repository_registry.get_artifact_repository(artifact_uri)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 74, in get_artifact_repository\r\n",
      "    return repository(artifact_uri)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 26, in __init__\r\n",
      "    uri = RunsArtifactRepository.get_underlying_uri(artifact_uri)\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 39, in get_underlying_uri\r\n",
      "    uri = get_artifact_uri(run_id, artifact_path, tracking_uri)\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/tracking/artifact_utils.py\", line 47, in get_artifact_uri\r\n",
      "    run = store.get_run(run_id)\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 659, in get_run\r\n",
      "    run_info = self._get_run_info(run_id)\r\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/muznymir/Library/Caches/pypoetry/virtualenvs/mole-dxJyzUfb-py3.11/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 683, in _get_run_info\r\n",
      "    raise MlflowException(\r\n",
      "mlflow.exceptions.MlflowException: Run 'ff5d0330792c4bf7b62e2609bbc4410c' not found\r\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": "!mlflow models serve -m runs:/ff5d0330792c4bf7b62e2609bbc4410c/model -p 1234 --enable-mlserver",
   "id": "96b602bfc16e373"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inference REST API endpoint can be called now by",
   "id": "4645ce361c2ebdea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!curl -X POST -d '{\"inputs\":[{\"query\": \"utterly stupid\"}]}' http://localhost:1234/invocations",
   "id": "8ad94b77e0118f02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deployment to Hubro\n",
    "\n",
    "Deployment to Hubro\n",
    "\n",
    "#### KNative\n",
    "\n",
    "KNative makes use of prepared docker images.  "
   ],
   "id": "59cd214ded9a2567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!docker login -u USERNAME -p PASSWORD ",
   "id": "f333d87ac48152d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!mlflow deployments create --name replicate_answering_model -t knative:/ --model-uri wasbs://models@hubro.blob.core.windows.net/ --config image_repository=docker.io/mmuzny/replicate-test-final --config image_tag=latest --config service_template=service_template.yaml",
   "id": "7ef4b65bfcee6144"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
